{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a58890c",
   "metadata": {},
   "source": [
    "Much of the code was inspired by the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625b39c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f476593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\3867474407.py:8: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, val in df1['Sentiment'].iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\n",
      "positive values: 248575\n",
      "negative values: 800000\n",
      "missing values: 0\n",
      "total:1048575\n",
      "----------------------------\n",
      "Testing dataset\n",
      "positive values: 182\n",
      "negative values: 177\n",
      "missing values: 0\n",
      "total:359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\3867474407.py:24: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, val in df2['Sentiment'].iteritems():\n"
     ]
    }
   ],
   "source": [
    "# step 1: Exploratory Data Analysis\n",
    "df1 = pd.read_csv(\"datasets/train.csv\")\n",
    "df2 = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "missing_count = 0\n",
    "for idx, val in df1['Sentiment'].iteritems():\n",
    "    if val == 0:\n",
    "        neg_count += 1\n",
    "    elif val == 1:\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        missing_count += 1\n",
    "print(\"Training dataset\")\n",
    "print(\"positive values: \" + str(pos_count))\n",
    "print(\"negative values: \" + str(neg_count))\n",
    "print(\"missing values: \" + str(missing_count))\n",
    "print(\"total:\" + str(len(df1)))\n",
    "print(\"----------------------------\")\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "missing_count = 0\n",
    "for idx, val in df2['Sentiment'].iteritems():\n",
    "    if val == 0:\n",
    "        neg_count += 1\n",
    "    elif val == 1:\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        missing_count += 1\n",
    "print(\"Testing dataset\")\n",
    "print(\"positive values: \" + str(pos_count))\n",
    "print(\"negative values: \" + str(neg_count))\n",
    "print(\"missing values: \" + str(missing_count))\n",
    "print(\"total:\" + str(len(df2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd7b385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:7: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, val in df1['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:13: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df1['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:19: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df1['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:37: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df1['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:52: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df1['Text'].iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:66: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df1['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:78: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, val in df2['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:84: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df2['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:90: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df2['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:108: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df2['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:121: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df2['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\1804388597.py:134: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, values in df2['Text'].iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Text Preprocessing\n",
    "\n",
    "df1 = df1.sample(n=1000, random_state=42)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "# df1 = pd.DataFrame({'Text': [\"good movie\", \"not a good movie\", \"did not like\"]}) #used for testing\n",
    "# Sentence segmentation\n",
    "for idx, val in df1['Text'].iteritems():\n",
    "    df1.at[idx, 'Text'] = sent_tokenize(df1.at[idx, 'Text'])\n",
    "    print()\n",
    "\n",
    "\n",
    "# make all characters lowercase\n",
    "for idx, values in df1['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        cur = values[i]\n",
    "        df1.at[idx, 'Text'][i] = cur.lower()\n",
    "\n",
    "# Removes contractions\n",
    "for idx, values in df1['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        text = values[i]\n",
    "        # text = df1.at[idx, 'Text']\n",
    "        text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        df1.at[idx, 'Text'][i] = text\n",
    "\n",
    "\n",
    "# Only alphabetical chars and spaces\n",
    "for idx, values in df1['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        new_val = []\n",
    "        val = values[i]\n",
    "        for c in val:\n",
    "            if (ord(c) >= 97 and ord(c) <= 122) or c == ' ':\n",
    "                new_val.append(c)\n",
    "            # else:\n",
    "                # new_val.append(\" \") # todo revise later\n",
    "\n",
    "        df1.at[idx, 'Text'][i] = \"\".join(new_val)\n",
    "\n",
    "\n",
    "# stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "for idx, values in df1['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        new_sentence = []\n",
    "        sentence = values[i].split(\" \")\n",
    "        for word in sentence:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            new_sentence.append(stemmed_word)\n",
    "\n",
    "        df1.at[idx, 'Text'][i] = \" \".join(new_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reverse sentence segmentation\n",
    "for idx, values in df1['Text'].iteritems():\n",
    "    df1.at[idx, 'Text'] = \" \".join(df1.at[idx, 'Text'])\n",
    "\n",
    "\n",
    "# DF2\n",
    "# Step 2: Text Preprocessing\n",
    "\n",
    "df2 = df2.sample(n=200, random_state=42)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "# df2 = pd.DataFrame({'Text': [\"good movie\", \"not a good movie\", \"did not like\"]}) #used for testing\n",
    "# Sentence segmentation\n",
    "for idx, val in df2['Text'].iteritems():\n",
    "    df2.at[idx, 'Text'] = sent_tokenize(df2.at[idx, 'Text'])\n",
    "    print()\n",
    "\n",
    "\n",
    "# make all characters lowercase\n",
    "for idx, values in df2['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        cur = values[i]\n",
    "        df2.at[idx, 'Text'][i] = cur.lower()\n",
    "\n",
    "# Removes contractions\n",
    "for idx, values in df2['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        text = values[i]\n",
    "        # text = df2.at[idx, 'Text']\n",
    "        text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        df2.at[idx, 'Text'][i] = text\n",
    "\n",
    "\n",
    "# Only alphabetical chars and spaces\n",
    "for idx, values in df2['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        new_val = []\n",
    "        val = values[i]\n",
    "        for c in val:\n",
    "            if (ord(c) >= 97 and ord(c) <= 122) or c == ' ':\n",
    "                new_val.append(c)\n",
    "\n",
    "        df2.at[idx, 'Text'][i] = \"\".join(new_val)\n",
    "\n",
    "\n",
    "# stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "for idx, values in df2['Text'].iteritems():\n",
    "    for i in range(len(values)):\n",
    "        new_sentence = []\n",
    "        sentence = values[i].split(\" \")\n",
    "        for word in sentence:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            new_sentence.append(stemmed_word)\n",
    "\n",
    "        df2.at[idx, 'Text'][i] = \" \".join(new_sentence) # todo might want to switch out so each cell is broken into words\n",
    "\n",
    "\n",
    "\n",
    "# reverse sentence segmentation\n",
    "for idx, values in df2['Text'].iteritems():\n",
    "    df2.at[idx, 'Text'] = \" \".join(df2.at[idx, 'Text'])\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d45abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\2395915059.py:4: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, sentence in df1['Text'].iteritems():\n",
      "C:\\Users\\orlan\\AppData\\Local\\Temp\\ipykernel_16096\\2395915059.py:16: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, sentence in df1['Text'].iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "# calculates the number of times each word appears \n",
    "word2count = {}\n",
    "for idx, sentence in df1['Text'].iteritems():\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "unique_words = list(word2count.keys())\n",
    "\n",
    "bag_of_words = []\n",
    "# fills in bag of words matrix\n",
    "for idx, sentence in df1['Text'].iteritems():\n",
    "    words = nltk.word_tokenize(sentence) # breaks sentence down into words\n",
    "    bag_vector = np.zeros(len(unique_words))\n",
    "    for w in words:\n",
    "        for index, word in enumerate(unique_words):\n",
    "            if word == w:\n",
    "                bag_vector[index] += 1\n",
    "    bag_of_words.append(bag_vector)\n",
    "\n",
    "x_train = np.array(bag_of_words)\n",
    "y_train = df1[\"Sentiment\"]\n",
    "\n",
    "bag_of_words_test = []\n",
    "\n",
    "for sentence in df2['Text']:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    bag_vector = np.zeros(len(unique_words))\n",
    "    for w in words:\n",
    "        for index, word in enumerate(unique_words):\n",
    "            if word == w:\n",
    "                bag_vector[index] += 1\n",
    "    bag_of_words_test.append(bag_vector)\n",
    "\n",
    "x_test = np.array(bag_of_words_test)\n",
    "y_test = df2[\"Sentiment\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123a6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticRegression()\n",
    "svc = SVC(probability=True)\n",
    "nbc = GaussianNB()\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3093ccdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.fit(x_train, y_train)\n",
    "svc.fit(x_train, y_train)\n",
    "nbc.fit(x_train, y_train)\n",
    "rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95df704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lc_predicted = lc.predict(x_test)\n",
    "y_lc_pred_proba = lc.predict_proba(x_test)\n",
    "\n",
    "y_svc_predicted = svc.predict(x_test)\n",
    "y_svc_pred_proba = svc.predict_proba(x_test)\n",
    "\n",
    "y_nbc_predicted = nbc.predict(x_test)\n",
    "y_nbc_pred_proba = nbc.predict_proba(x_test)\n",
    "\n",
    "y_rfc_predicted = rfc.predict(x_test)\n",
    "y_rfc_pred_proba = rfc.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fc7940",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.93      0.73       107\n",
      "           1       0.78      0.30      0.43        93\n",
      "\n",
      "    accuracy                           0.64       200\n",
      "   macro avg       0.69      0.61      0.58       200\n",
      "weighted avg       0.68      0.64      0.59       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       107\n",
      "           1       1.00      0.02      0.04        93\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.77      0.51      0.37       200\n",
      "weighted avg       0.75      0.55      0.39       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.77      0.65       107\n",
      "           1       0.55      0.33      0.42        93\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.56      0.55      0.53       200\n",
      "weighted avg       0.56      0.56      0.54       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       107\n",
      "           1       1.00      0.03      0.06        93\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.77      0.52      0.38       200\n",
      "weighted avg       0.76      0.55      0.41       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_lc_predicted))\n",
    "print(classification_report(y_test, y_svc_predicted))\n",
    "print(classification_report(y_test, y_nbc_predicted))\n",
    "print(classification_report(y_test, y_rfc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f16ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idf_model = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(df1[\"Text\"]) # vectorizes text column\n",
    "tf_idf_vector = tf_idf_vector.toarray()\n",
    "\n",
    "# Use the same vectorizer to transform the test data\n",
    "tf_idf_vector_test = tr_idf_model.transform(df2[\"Text\"])\n",
    "tf_idf_vector_test = tf_idf_vector_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de18c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf_idf_vector\n",
    "y_train = df1[\"Sentiment\"]\n",
    "\n",
    "x_test = tf_idf_vector_test\n",
    "y_test = df2[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4842e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticRegression()\n",
    "svc = SVC(probability=True)\n",
    "nbc = GaussianNB()\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e35dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.fit(x_train, y_train)\n",
    "svc.fit(x_train, y_train)\n",
    "nbc.fit(x_train, y_train)\n",
    "rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a06791",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lc_predicted = lc.predict(x_test)\n",
    "y_lc_pred_proba = lc.predict_proba(x_test)\n",
    "\n",
    "y_svc_predicted = svc.predict(x_test)\n",
    "y_svc_pred_proba = svc.predict_proba(x_test)\n",
    "\n",
    "y_nbc_predicted = nbc.predict(x_test)\n",
    "y_nbc_pred_proba = nbc.predict_proba(x_test)\n",
    "\n",
    "y_rfc_predicted = rfc.predict(x_test)\n",
    "y_rfc_pred_proba = rfc.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f6de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       107\n",
      "           1       1.00      0.01      0.02        93\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.77      0.51      0.36       200\n",
      "weighted avg       0.75      0.54      0.38       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       107\n",
      "           1       1.00      0.01      0.02        93\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.77      0.51      0.36       200\n",
      "weighted avg       0.75      0.54      0.38       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65       107\n",
      "           1       0.54      0.34      0.42        93\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.55      0.55      0.53       200\n",
      "weighted avg       0.56      0.56      0.54       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72       107\n",
      "           1       1.00      0.11      0.19        93\n",
      "\n",
      "    accuracy                           0.58       200\n",
      "   macro avg       0.78      0.55      0.46       200\n",
      "weighted avg       0.77      0.58      0.48       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_lc_predicted))\n",
    "print(classification_report(y_test, y_svc_predicted))\n",
    "print(classification_report(y_test, y_nbc_predicted))\n",
    "print(classification_report(y_test, y_rfc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c3c8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec, code inspired by https://spotintelligence.com/2023/02/15/word2vec-for-text-classification/\n",
    "x_train = [str(sentence).split() for sentence in df1['Text']]\n",
    "x_test = [str(sentence).split() for sentence in df2['Text']]\n",
    "y_train = list(df1[\"Sentiment\"])\n",
    "y_test = list(df2[\"Sentiment\"])\n",
    "w2v_model = Word2Vec(x_train, window=5, min_count=5, workers=4)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "x_train = np.array([vectorize(sentence) for sentence in x_train])\n",
    "x_test = np.array([vectorize(sentence) for sentence in x_test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d0f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticRegression()\n",
    "svc = SVC(probability=True)\n",
    "nbc = GaussianNB()\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e218a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.fit(x_train, y_train)\n",
    "svc.fit(x_train, y_train)\n",
    "nbc.fit(x_train, y_train)\n",
    "rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72ce45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lc_predicted = lc.predict(x_test)\n",
    "y_lc_pred_proba = lc.predict_proba(x_test)\n",
    "\n",
    "y_svc_predicted = svc.predict(x_test)\n",
    "y_svc_pred_proba = svc.predict_proba(x_test)\n",
    "\n",
    "y_nbc_predicted = nbc.predict(x_test)\n",
    "y_nbc_pred_proba = nbc.predict_proba(x_test)\n",
    "\n",
    "y_rfc_predicted = rfc.predict(x_test)\n",
    "y_rfc_pred_proba = rfc.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a39efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       107\n",
      "           1       0.00      0.00      0.00        93\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.27      0.50      0.35       200\n",
      "weighted avg       0.29      0.54      0.37       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       107\n",
      "           1       0.00      0.00      0.00        93\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.27      0.50      0.35       200\n",
      "weighted avg       0.29      0.54      0.37       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.70      0.61       107\n",
      "           1       0.48      0.31      0.38        93\n",
      "\n",
      "    accuracy                           0.52       200\n",
      "   macro avg       0.51      0.51      0.49       200\n",
      "weighted avg       0.51      0.52      0.50       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.95      0.68       107\n",
      "           1       0.29      0.02      0.04        93\n",
      "\n",
      "    accuracy                           0.52       200\n",
      "   macro avg       0.41      0.49      0.36       200\n",
      "weighted avg       0.42      0.52      0.38       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orlan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orlan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orlan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orlan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orlan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orlan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_lc_predicted))\n",
    "print(classification_report(y_test, y_svc_predicted))\n",
    "print(classification_report(y_test, y_nbc_predicted))\n",
    "print(classification_report(y_test, y_rfc_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b6e33",
   "metadata": {},
   "source": [
    "MODEL EVALUATION: \n",
    "All models performed quite similarly. however it seems like bag-of-words performed with the highest accuracy followed by tf*idf, and then word2vector. For bag-of-words, the logistic regression model performed the best, for the tf*idf, the random forest classifier performed the best, and for the word2vector, the support vector machine and logistic regression model tied for most accurate. The model with the highest accuracy was the logisitc regression model for the bag_of_words which was only 64 percent accurate. This could definitely be improve, particularly in the preprocessing stage. If lemmatization was used, there could be a notable increase in accuracy. Moreover, if I used more datapoints, the might also be an increase in accuracy. Lastly, there might be some inaccuracies in my code which could be hindering the results as well.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
